{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import pca\n",
    "from sklearn.metrics import (f1_score, roc_auc_score, roc_curve, auc, confusion_matrix, precision_recall_curve, \n",
    "                             average_precision_score, classification_report, accuracy_score)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateStandardTimeSeriesStructure(all_releases_df, ws):\n",
    " \n",
    "    print(\"Generating a new dataframe without containing the last release...\")\n",
    "    df = all_releases_df[all_releases_df['releaseID'] != all_releases_df['releaseID'].max()]\n",
    "    print(\"... DONE!\")\n",
    "\n",
    "    df.drop(columns=['instanceID', 'releaseID', \n",
    "                                  'class_frequency', 'number_of_changes', 'will_change', 'change_probability'])\n",
    "    \n",
    "    print(\"checking class larger than window size...\")\n",
    "\n",
    "    window_size=ws;\n",
    "\n",
    "    class_names_list = df['Path'].unique().tolist()\n",
    "    classes_to_drop_list = list()\n",
    "    for class_name in class_names_list:\n",
    "        if len(df[df.Path == class_name].iloc[::-1]) <= window_size:\n",
    "            for drop_class in df.index[df.Path==class_name].tolist():\n",
    "                classes_to_drop_list.append(drop_class)\n",
    "\n",
    "\n",
    "    df = df.drop(classes_to_drop_list, axis=0)\n",
    "    df = df.iloc[::-1]\n",
    "\n",
    "    print(\"DONE\")\n",
    "    \n",
    "    print(\"Setting the features...\")\n",
    "    class_names_list = df['Path'].unique().tolist()\n",
    "    features_list = ['CountClassCoupled', 'CountClassDerived', 'CountDeclMethod', 'CountDeclMethodAll', 'CountLineCode', 'MaxInheritanceTree', 'PercentLackOfCohesion', 'SumCyclomatic']\n",
    "    print(\"DONE\")\n",
    "    \n",
    "    timeseries_list = list()\n",
    "    timeseries_labels = list()\n",
    "    for class_name in class_names_list:\n",
    "        class_sequence = df[df.Path == class_name].reset_index()\n",
    "        for row in range(len(class_sequence)-1):\n",
    "            window = list()\n",
    "            # print('row: ', row)\n",
    "            if row + window_size < len(class_sequence) + 1:\n",
    "                for i in range(window_size):\n",
    "                    #print(row+i)\n",
    "                    window.extend(class_sequence.loc[row + i, features_list].values.astype(np.float64))\n",
    "                timeseries_labels.append(class_sequence.loc[row + i, 'will_change'])\n",
    "                timeseries_list.append(window)\n",
    "                \n",
    "    timeseries_X = np.array(timeseries_list)\n",
    "    timeseries_labels = np.array(timeseries_labels).astype(np.bool)\n",
    "    \n",
    "    print(\"X:\", timeseries_X.shape, \"y:\", timeseries_labels.shape)\n",
    "    \n",
    "    return timeseries_X, timeseries_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_test, y_pred):\n",
    "    scores = []\n",
    "    \n",
    "    scores.append(f1_score(y_test, y_pred, average='micro'))\n",
    "    print(\"F1-Score(micro): \" + str(scores[-1]))\n",
    "    \n",
    "    scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    print(\"F1-Score(macro): \" + str(scores[-1]))\n",
    "    \n",
    "    scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"F1-Score(weighted): \" + str(scores[-1]))\n",
    "    \n",
    "    scores.append(f1_score(y_test, y_pred, average=None))\n",
    "    print(\"F1-Score(None): \" + str(scores[-1]))\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    #ACC\n",
    "    scores.append(accuracy_score(y_test, y_pred, normalize=True))\n",
    "    print(\"Accuracy: \" + str(scores[-1]))\n",
    "    \n",
    "    #Sensitivity\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    scores.append(tp / (tp+fn))\n",
    "    print(\"Sensitivity: \" + str(scores[-1]))\n",
    "    \n",
    "    #Specificity\n",
    "    specificity = tn / (tn+fp)\n",
    "    scores.append (tn / (tn+fp))\n",
    "    print(\"Specificity: \" + str(scores[-1]))\n",
    "    \n",
    "    #VPP\n",
    "    scores.append(tp / (tp+fp))\n",
    "    #print(\"VPP: \" + str(scores[-1]))\n",
    "    \n",
    "    #VPN\n",
    "    scores.append(tn / (tn+fn))\n",
    "    #print(\"VPN: \" + str(scores[-1]))\n",
    "    \n",
    "    #RVP\n",
    "    scores.append(sensitivity / (1-specificity))\n",
    "    #print(\"RVP: \" + str(scores[-1]))\n",
    "    \n",
    "    #RVN\n",
    "    scores.append((1 - sensitivity) / specificity)\n",
    "    #print(\"RVN: \" + str(scores[-1]))\n",
    "    \n",
    "    #Confusion Matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Confusion Matrix: [\" + str(cnf_matrix[0][0]) + \", \" + str(round(cnf_matrix[1][1],2)) + \"]\")\n",
    "    \n",
    "    #ROC_AUC\n",
    "    scores.append(roc_auc_score(y_test, y_pred))\n",
    "    print(\"ROC AUC score: \" + str(scores[-1]))\n",
    "        \n",
    "    scores.append([tn, fp, fn, tp])\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    #    print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    #tick_marks = np.arange(len(classes))\n",
    "    #plt.xticks(tick_marks, classes, rotation=45)\n",
    "    #plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_confusion_matrixes(y_test, y_pred):\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    #plt.figure()\n",
    "    #plt.subplots(1,2,figsize=(20,4))\n",
    "    #plt.subplot(1,2,1)\n",
    "    #plot_confusion_matrix(cnf_matrix, title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    #plt.subplot(1,2,2)\n",
    "    plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegr_(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    print(\"\\nLOGISTIC REGRESSION\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_LR, xvl_LR = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        ytr_LR, yvl_LR = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        #model\n",
    "        lr = LogisticRegression(solver='lbfgs', random_state=42, class_weight='balanced')\n",
    "        lr.fit(xtr_LR, ytr_LR.values.ravel())\n",
    "        score = roc_auc_score(yvl_LR, lr.predict(xvl_LR))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))\n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, lr.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree_(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"\\nDECISION TREE\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_DT, xvl_DT = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        ytr_DT, yvl_DT = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        #model\n",
    "        dt = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "        dt.fit(xtr_DT, ytr_DT.values.ravel())\n",
    "        score = roc_auc_score(yvl_DT, dt.predict(xvl_DT))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))    \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, dt.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"RANDOM FOREST\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_RF, xvl_RF = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        ytr_RF, yvl_RF = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        #model\n",
    "        rf = RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100)\n",
    "        rf.fit(xtr_RF, ytr_RF.values.ravel())\n",
    "        score = roc_auc_score(yvl_RF, rf.predict(xvl_RF))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))    \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, rf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "\n",
    "    print(\"NEURAL NETWORK\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_NN, xvl_NN = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        ytr_NN, yvl_NN = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        #model\n",
    "        nn = MLPClassifier(random_state=42)\n",
    "        nn.fit(xtr_NN, ytr_NN.values.ravel())\n",
    "        score = roc_auc_score(yvl_NN, nn.predict(xvl_NN))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))   \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, nn.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms (no iloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegr_NoIloc(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"\\nLOGISTIC REGRESSION\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_LR, xvl_LR = Xtrain[train_index], Xtrain[test_index]\n",
    "        ytr_LR, yvl_LR = Ytrain[train_index], Ytrain[test_index]\n",
    "\n",
    "        #model\n",
    "        lr = LogisticRegression(solver='lbfgs', random_state=42, class_weight='balanced')\n",
    "        lr.fit(xtr_LR, ytr_LR)\n",
    "        score = roc_auc_score(yvl_LR, lr.predict(xvl_LR))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))\n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, lr.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree_NoIloc(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"\\nDECISION TREE\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_DT, xvl_DT = Xtrain[train_index], Xtrain[test_index]\n",
    "        ytr_DT, yvl_DT = Ytrain[train_index], Ytrain[test_index]\n",
    "        #model\n",
    "        dt = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "        dt.fit(xtr_DT, ytr_DT)\n",
    "        score = roc_auc_score(yvl_DT, dt.predict(xvl_DT))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))    \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, dt.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_NoIloc(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"RANDOM FOREST\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_RF, xvl_RF = Xtrain[train_index], Xtrain[test_index]\n",
    "        ytr_RF, yvl_RF = Ytrain[train_index], Ytrain[test_index]\n",
    "\n",
    "        #model\n",
    "        rf = RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100)\n",
    "        rf.fit(xtr_RF, ytr_RF)\n",
    "        score = roc_auc_score(yvl_RF, rf.predict(xvl_RF))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))    \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, rf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_NoIloc(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "\n",
    "    print(\"NEURAL NETWORK\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_NN, xvl_NN = Xtrain[train_index], Xtrain[test_index]\n",
    "        ytr_NN, yvl_NN = Ytrain[train_index], Ytrain[test_index]\n",
    "\n",
    "        #model\n",
    "        nn = MLPClassifier(random_state=42)\n",
    "        nn.fit(xtr_NN, ytr_NN)\n",
    "        score = roc_auc_score(yvl_NN, nn.predict(xvl_NN))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))   \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, nn.predict(Xtest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
