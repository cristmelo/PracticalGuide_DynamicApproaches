{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cristiano.melo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(y_test, y_pred, weight_t):\n",
    "    print(\"ROC_AUC: \" + str(roc_auc_score(y_test, y_pred, sample_weight=weights_t)))\n",
    "    print(\"F1-Score: \" + str(f1_score(y_test, y_pred, sample_weight=weights_t)))\n",
    "    print(\"Precision: \" + str(precision_score(y_test, y_pred, sample_weight=weights_t)))\n",
    "    print(\"Recall: \" + str(recall_score(y_test, y_pred, sample_weight=weights_t)))\n",
    "    print(\"Accuracy: \" + str(accuracy_score(y_test, y_pred, sample_weight=weights_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_GRU(input_shape, output_dim, dropout=0.3):\n",
    "    print(\"model dim: \", input_shape, output_dim)\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(64))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Tensor according to predefined Window Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSeriesWindow(window_size, df):\n",
    "    \n",
    "    print(\"generating tensor...\")\n",
    "    class_names_list = df['Path'].unique().tolist()\n",
    "    classes_to_drop_list = list()\n",
    "    for class_name in class_names_list:\n",
    "        if len(df[df.Path == class_name].iloc[::-1]) <= window_size:\n",
    "            for drop_class in df.index[df.Path==class_name].tolist():\n",
    "                classes_to_drop_list.append(drop_class)\n",
    "    \n",
    "\n",
    "    df = df.drop(classes_to_drop_list, axis=0)\n",
    "    df = df.iloc[::-1]\n",
    "\n",
    "    class_names_list = df['Path'].unique().tolist()\n",
    "    features_list = ['CountClassCoupled', 'CountClassDerived', 'CountDeclMethod', 'CountDeclMethodAll', 'CountLineCode', 'MaxInheritanceTree', 'PercentLackOfCohesion', 'SumCyclomatic']\n",
    "\n",
    "    timeseries_list = list()\n",
    "    timeseries_labels = list()\n",
    "    for class_name in class_names_list:\n",
    "        class_sequence = df[df.Path == class_name].reset_index()\n",
    "        for row in range(len(class_sequence)-1):\n",
    "            window = list()\n",
    "            # print('row: ', row)\n",
    "            if row + window_size < len(class_sequence) + 1:\n",
    "                for i in range(window_size):\n",
    "                    #print(row+i)\n",
    "                    window.append(class_sequence.loc[row + i, features_list].values.astype(np.float64))\n",
    "                timeseries_labels.append(class_sequence.loc[row + i, 'will_change'])\n",
    "                timeseries_list.append(window)\n",
    "    \n",
    "\n",
    "    timeseries_tensor = np.array(timeseries_list)\n",
    "\n",
    "    timeseries_tensor = timeseries_tensor.transpose((0,2,1))\n",
    "\n",
    "    timeseries_labels = np.array(timeseries_labels).astype(np.bool)\n",
    "\n",
    "    timeseries_labels = timeseries_labels.reshape(-1, 1)\n",
    "\n",
    "    will_change_labels = timeseries_labels.astype(np.int)\n",
    "\n",
    "    not_will_change = np.invert(timeseries_labels).astype(np.int)\n",
    "\n",
    "    timeseries_labels = np.concatenate((will_change_labels, not_will_change), axis=1)\n",
    "\n",
    "    print(\"...DONE!\")\n",
    "    \n",
    "    print(\"tensor dimensions:\")\n",
    "    print(\"tensor input X:\", timeseries_tensor.shape)\n",
    "\n",
    "    print(\"tensor input y:\", timeseries_labels.shape)\n",
    "  \n",
    "    print(\"proportion of y labels:\", np.unique(not_will_change, return_counts=True))\n",
    "        \n",
    "    print(\"Splitting dataset into Train and Test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(timeseries_tensor, timeseries_labels, test_size=0.30, random_state=42)\n",
    "    X_train = X_train.transpose((0,2,1))\n",
    "    X_test = X_test.transpose((0,2,1))\n",
    "    print(\"Tensor X train:\", X_train.shape)\n",
    "    print(\"Tensor y train:\", y_train.shape)\n",
    "    print(\"Tensor X test:\", X_test.shape)\n",
    "    print(\"Tensor y test:\", y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test  \n",
    "\n",
    "\n",
    "def applyTensor(window_size, df):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = getTimeSeriesWindow(window_size, df)\n",
    " \n",
    "    print(\"computing weights...\")\n",
    "    \n",
    "    fractions = 1-y_train.sum(axis=0)/len(y_train)\n",
    "    weights = fractions[y_train.argmax(axis=1)]\n",
    "    print(\"... DONE!\")\n",
    "\n",
    "    print(\"setting stratified k-fold...\")\n",
    "    k=5\n",
    "    print(\"number of k:\",k)\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True,  random_state=1337)\n",
    "    print(\"... DONE!\")\n",
    "    \n",
    "    print(\"Executing algorithm...\")\n",
    "    \n",
    "    lastModels = {}\n",
    "    history_general = {}\n",
    "    val_history_general = {}\n",
    "    \n",
    "    set_number_of_epochs=20\n",
    "    set_batch_size=512\n",
    "    print(\"number of epochs:\", set_number_of_epochs)\n",
    "    print(\"number of batch:\", set_batch_size)\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(X_train, y_train[:,0])):\n",
    "        print (\"Training on fold \" + str(index + 1) + \"/\"+str(k)+\"...\") \n",
    "        xtrain, xval = X_train[train_indices], X_train[val_indices]\n",
    "        ytrain, yval = y_train[train_indices], y_train[val_indices] \n",
    "        weights_train = weights[train_indices]\n",
    "        weights_val = weights[val_indices]\n",
    "\n",
    "        model = None\n",
    "        model = make_GRU((xtrain.shape[1], xtrain.shape[2]), 2)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', auc, f1])\n",
    "\n",
    "        history = model.fit(xtrain, ytrain, validation_data=(xval, yval, weights_val), epochs=set_number_of_epochs, batch_size=set_batch_size, sample_weight=weights_train)\n",
    "\n",
    "\n",
    "        output = model.predict_classes(xval)\n",
    "\n",
    "        print(confusion_matrix(yval.argmax(axis=1), output))\n",
    "\n",
    "        print(classification_report(yval.argmax(axis=1), output))\n",
    "\n",
    "\n",
    "    lastModel = model\n",
    "    history_general = history.history\n",
    "    \n",
    "    print(\"... DONE!\")\n",
    "    \n",
    "    return [lastModel , history_general, X_test, y_test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
