{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cristiano.melo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import pca\n",
    "from sklearn.metrics import (f1_score, roc_auc_score, roc_curve, auc, confusion_matrix, precision_recall_curve, \n",
    "                             average_precision_score, classification_report, accuracy_score)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_test, y_pred):\n",
    "    scores = []\n",
    "    \n",
    "    scores.append(f1_score(y_test, y_pred, average='micro'))\n",
    "    print(\"F1-Score(micro): \" + str(scores[-1]))\n",
    "    \n",
    "    scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    print(\"F1-Score(macro): \" + str(scores[-1]))\n",
    "    \n",
    "    scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"F1-Score(weighted): \" + str(scores[-1]))\n",
    "    \n",
    "    scores.append(f1_score(y_test, y_pred, average=None))\n",
    "    print(\"F1-Score(None): \" + str(scores[-1]))\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    #ACC\n",
    "    scores.append(accuracy_score(y_test, y_pred, normalize=True))\n",
    "    print(\"Accuracy: \" + str(scores[-1]))\n",
    "    \n",
    "    #Sensitivity\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    scores.append(tp / (tp+fn))\n",
    "    print(\"Sensitivity: \" + str(scores[-1]))\n",
    "    \n",
    "    #Specificity\n",
    "    specificity = tn / (tn+fp)\n",
    "    scores.append (tn / (tn+fp))\n",
    "    print(\"Specificity: \" + str(scores[-1]))\n",
    "    \n",
    "    #VPP\n",
    "    scores.append(tp / (tp+fp))\n",
    "    #print(\"VPP: \" + str(scores[-1]))\n",
    "    \n",
    "    #VPN\n",
    "    scores.append(tn / (tn+fn))\n",
    "    #print(\"VPN: \" + str(scores[-1]))\n",
    "    \n",
    "    #RVP\n",
    "    scores.append(sensitivity / (1-specificity))\n",
    "    #print(\"RVP: \" + str(scores[-1]))\n",
    "    \n",
    "    #RVN\n",
    "    scores.append((1 - sensitivity) / specificity)\n",
    "    #print(\"RVN: \" + str(scores[-1]))\n",
    "    \n",
    "    #Confusion Matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Confusion Matrix: [\" + str(cnf_matrix[0][0]) + \", \" + str(round(cnf_matrix[1][1],2)) + \"]\")\n",
    "    \n",
    "    #ROC_AUC\n",
    "    scores.append(roc_auc_score(y_test, y_pred))\n",
    "    print(\"ROC AUC score: \" + str(scores[-1]))\n",
    "        \n",
    "    scores.append([tn, fp, fn, tp])\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_AUC(xv1,yv1):\n",
    "    proba = lr.predict_proba(xvl)[:,1]\n",
    "    frp,trp, threshold = roc_curve(yvl,proba)\n",
    "    roc_auc_ = auc(frp,trp)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Reciever Operating Characteristics')\n",
    "    plt.plot(frp,trp,'r',label = 'AUC = %0.2f' % roc_auc_)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'b--')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.xlabel('False positive rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    #    print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    #tick_marks = np.arange(len(classes))\n",
    "    #plt.xticks(tick_marks, classes, rotation=45)\n",
    "    #plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_confusion_matrixes(y_test, y_pred):\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    #plt.figure()\n",
    "    #plt.subplots(1,2,figsize=(20,4))\n",
    "    #plt.subplot(1,2,1)\n",
    "    #plot_confusion_matrix(cnf_matrix, title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    #plt.subplot(1,2,2)\n",
    "    plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_counter(df):\n",
    "\n",
    "    outliers_index_list = list()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        outlier_counter = 0\n",
    "        if row.CountClassCoupled > boxplot_max_list[0]:\n",
    "            outlier_counter = outlier_counter + 1\n",
    "\n",
    "        if row.SumCyclomatic > boxplot_max_list[1]:\n",
    "            outlier_counter = outlier_counter + 1\n",
    "\n",
    "        if row.MaxInheritanceTree > boxplot_max_list[2]:\n",
    "            outlier_counter = outlier_counter + 1\n",
    "\n",
    "        if row.PercentLackOfCohesion > boxplot_max_list[3]:\n",
    "            outlier_counter = outlier_counter + 1\n",
    "\n",
    "        if row.CountLineCode > boxplot_max_list[4]:\n",
    "            outlier_counter = outlier_counter + 1\n",
    "\n",
    "        if row.CountClassDerived > boxplot_max_list[5]:\n",
    "            outlier_counter = outlier_counter + 1\n",
    "\n",
    "        if row.CountDeclMethodAll > boxplot_max_list[6]:\n",
    "            outlier_counter = outlier_counter + 1\n",
    "\n",
    "        if row.CountDeclMethod > boxplot_max_list[7]:\n",
    "            outlier_counter = outlier_counter + 1\n",
    "\n",
    "        if outlier_counter >= 4:\n",
    "            outliers_index_list.append(index)\n",
    "    \n",
    "    return outliers_index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegr_(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    print(\"\\nLOGISTIC REGRESSION\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_LR, xvl_LR = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        ytr_LR, yvl_LR = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        #model\n",
    "        lr = LogisticRegression(solver='lbfgs', random_state=42, class_weight='balanced')\n",
    "        lr.fit(xtr_LR, ytr_LR.values.ravel())\n",
    "        score = roc_auc_score(yvl_LR, lr.predict(xvl_LR))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))\n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, lr.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree_(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"\\nDECISION TREE\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_DT, xvl_DT = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        ytr_DT, yvl_DT = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        #model\n",
    "        dt = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "        dt.fit(xtr_DT, ytr_DT.values.ravel())\n",
    "        score = roc_auc_score(yvl_DT, dt.predict(xvl_DT))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))    \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, dt.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"RANDOM FOREST\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_RF, xvl_RF = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        ytr_RF, yvl_RF = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        #model\n",
    "        rf = RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100)\n",
    "        rf.fit(xtr_RF, ytr_RF.values.ravel())\n",
    "        score = roc_auc_score(yvl_RF, rf.predict(xvl_RF))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))    \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, rf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "\n",
    "    print(\"NEURAL NETWORK\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_NN, xvl_NN = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        ytr_NN, yvl_NN = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        #model\n",
    "        nn = MLPClassifier(random_state=42)\n",
    "        nn.fit(xtr_NN, ytr_NN.values.ravel())\n",
    "        score = roc_auc_score(yvl_NN, nn.predict(xvl_NN))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))   \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, nn.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms (no iloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegr_NoIloc(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"\\nLOGISTIC REGRESSION\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_LR, xvl_LR = Xtrain[train_index], Xtrain[test_index]\n",
    "        ytr_LR, yvl_LR = Ytrain[train_index], Ytrain[test_index]\n",
    "\n",
    "        #model\n",
    "        lr = LogisticRegression(solver='lbfgs', random_state=42, class_weight='balanced')\n",
    "        lr.fit(xtr_LR, ytr_LR)\n",
    "        score = roc_auc_score(yvl_LR, lr.predict(xvl_LR))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))\n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, lr.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree_NoIloc(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"\\nDECISION TREE\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_DT, xvl_DT = Xtrain[train_index], Xtrain[test_index]\n",
    "        ytr_DT, yvl_DT = Ytrain[train_index], Ytrain[test_index]\n",
    "        #model\n",
    "        dt = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "        dt.fit(xtr_DT, ytr_DT)\n",
    "        score = roc_auc_score(yvl_DT, dt.predict(xvl_DT))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))    \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, dt.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_NoIloc(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "    print(\"RANDOM FOREST\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_RF, xvl_RF = Xtrain[train_index], Xtrain[test_index]\n",
    "        ytr_RF, yvl_RF = Ytrain[train_index], Ytrain[test_index]\n",
    "\n",
    "        #model\n",
    "        rf = RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100)\n",
    "        rf.fit(xtr_RF, ytr_RF)\n",
    "        score = roc_auc_score(yvl_RF, rf.predict(xvl_RF))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))    \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, rf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_NoIloc(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    %%time\n",
    "\n",
    "    print(\"NEURAL NETWORK\")\n",
    "    cv_score = []\n",
    "    i = 1\n",
    "    print(\"TRAIN AND VALIDATION SETS:\")\n",
    "    for train_index, test_index in kf.split(Xtrain, Ytrain):\n",
    "        print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "        xtr_NN, xvl_NN = Xtrain[train_index], Xtrain[test_index]\n",
    "        ytr_NN, yvl_NN = Ytrain[train_index], Ytrain[test_index]\n",
    "\n",
    "        #model\n",
    "        nn = MLPClassifier(random_state=42)\n",
    "        nn.fit(xtr_NN, ytr_NN)\n",
    "        score = roc_auc_score(yvl_NN, nn.predict(xvl_NN))\n",
    "        print('ROC AUC score:',score)\n",
    "        cv_score.append(score)    \n",
    "        i+=1\n",
    "\n",
    "    print('\\nCROSS VALIDANTION SUMMARY:')\n",
    "    print('Mean: ' + str(np.mean(cv_score)))\n",
    "    print('Std deviation: ' + str(np.std(cv_score)))   \n",
    "\n",
    "    print(\"\\nTEST SET:\")\n",
    "    get_scores(Ytest, nn.predict(Xtest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
